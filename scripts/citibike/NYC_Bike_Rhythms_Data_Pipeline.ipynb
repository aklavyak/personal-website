{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Bike Rhythms — Data Pipeline\n",
        "\n",
        "This notebook downloads and processes Citi Bike trip data to generate JSON files for the visualization.\n",
        "\n",
        "**Output files:**\n",
        "- `neighborhoods.json` — NYC neighborhood boundaries with metadata\n",
        "- `weekly-patterns.json` — Hourly ride patterns by neighborhood\n",
        "- `story-moments.json` — Pre-computed statistics for story beats\n",
        "- `metadata.json` — Data summary\n",
        "\n",
        "**Runtime:** ~15-20 minutes for full year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas geopandas shapely requests tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "from io import BytesIO\n",
        "from shapely.geometry import Point\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('output', exist_ok=True)\n",
        "os.makedirs('raw', exist_ok=True)\n",
        "\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download NYC Neighborhood Boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NTA (Neighborhood Tabulation Area) boundaries\n",
        "NTA_URL = 'https://data.cityofnewyork.us/api/geospatial/9nt8-h7nd?method=export&format=GeoJSON'\n",
        "\n",
        "print('Downloading NYC neighborhood boundaries...')\n",
        "nta = gpd.read_file(NTA_URL)\n",
        "print(f'Loaded {len(nta)} neighborhoods')\n",
        "\n",
        "# Preview\n",
        "nta[['ntaname', 'ntacode', 'boroname']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Citi Bike Trip Data (Full Year 2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "YEAR = 2025\n",
        "MONTHS = range(1, 13)  # All 12 months\n",
        "\n",
        "BASE_URL = 'https://s3.amazonaws.com/tripdata'\n",
        "\n",
        "def download_month(year, month):\n",
        "    \"\"\"Download and extract a single month of Citi Bike data.\"\"\"\n",
        "    filename = f'{year}{month:02d}-citibike-tripdata.zip'\n",
        "    url = f'{BASE_URL}/{filename}'\n",
        "    \n",
        "    print(f'Downloading {filename}...')\n",
        "    response = requests.get(url, stream=True)\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        print(f'  Failed to download {filename}')\n",
        "        return None\n",
        "    \n",
        "    # Extract CSV from ZIP\n",
        "    with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "        csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
        "        \n",
        "        # Read all CSVs in the ZIP (some months have multiple files)\n",
        "        dfs = []\n",
        "        for csv_file in csv_files:\n",
        "            with z.open(csv_file) as f:\n",
        "                df = pd.read_csv(f, low_memory=False)\n",
        "                dfs.append(df)\n",
        "        \n",
        "        combined = pd.concat(dfs, ignore_index=True)\n",
        "        print(f'  Loaded {len(combined):,} trips')\n",
        "        return combined\n",
        "\n",
        "# Download all months\n",
        "all_trips = []\n",
        "for month in MONTHS:\n",
        "    df = download_month(YEAR, month)\n",
        "    if df is not None:\n",
        "        all_trips.append(df)\n",
        "\n",
        "# Combine all months\n",
        "trips = pd.concat(all_trips, ignore_index=True)\n",
        "print(f'\\nTotal trips: {len(trips):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Cleaning & Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize column names (they vary between months)\n",
        "column_mapping = {\n",
        "    'ride_id': 'ride_id',\n",
        "    'rideable_type': 'rideable_type',\n",
        "    'started_at': 'started_at',\n",
        "    'ended_at': 'ended_at',\n",
        "    'start_station_name': 'start_station_name',\n",
        "    'start_station_id': 'start_station_id',\n",
        "    'end_station_name': 'end_station_name',\n",
        "    'end_station_id': 'end_station_id',\n",
        "    'start_lat': 'start_lat',\n",
        "    'start_lng': 'start_lng',\n",
        "    'end_lat': 'end_lat',\n",
        "    'end_lng': 'end_lng',\n",
        "    'member_casual': 'member_casual'\n",
        "}\n",
        "\n",
        "# Rename columns to standard names\n",
        "trips = trips.rename(columns={k: v for k, v in column_mapping.items() if k in trips.columns})\n",
        "\n",
        "# Parse timestamps\n",
        "trips['started_at'] = pd.to_datetime(trips['started_at'])\n",
        "trips['ended_at'] = pd.to_datetime(trips['ended_at'])\n",
        "\n",
        "# Extract time features\n",
        "trips['hour'] = trips['started_at'].dt.hour\n",
        "trips['day_of_week'] = trips['started_at'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "trips['month'] = trips['started_at'].dt.month\n",
        "trips['is_weekend'] = trips['day_of_week'].isin([5, 6])\n",
        "trips['is_member'] = trips['member_casual'] == 'member'\n",
        "\n",
        "# Remove trips with missing coordinates\n",
        "before = len(trips)\n",
        "trips = trips.dropna(subset=['start_lat', 'start_lng', 'end_lat', 'end_lng'])\n",
        "print(f'Removed {before - len(trips):,} trips with missing coordinates')\n",
        "print(f'Remaining trips: {len(trips):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Spatial Join: Map Stations to Neighborhoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unique stations\n",
        "start_stations = trips[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()\n",
        "start_stations.columns = ['station_id', 'station_name', 'lat', 'lng']\n",
        "\n",
        "end_stations = trips[['end_station_id', 'end_station_name', 'end_lat', 'end_lng']].drop_duplicates()\n",
        "end_stations.columns = ['station_id', 'station_name', 'lat', 'lng']\n",
        "\n",
        "stations = pd.concat([start_stations, end_stations]).drop_duplicates(subset='station_id')\n",
        "print(f'Unique stations: {len(stations):,}')\n",
        "\n",
        "# Create GeoDataFrame\n",
        "stations_gdf = gpd.GeoDataFrame(\n",
        "    stations,\n",
        "    geometry=gpd.points_from_xy(stations.lng, stations.lat),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "# Spatial join to NTA\n",
        "stations_with_nta = gpd.sjoin(\n",
        "    stations_gdf,\n",
        "    nta[['ntaname', 'ntacode', 'boroname', 'geometry']],\n",
        "    how='left',\n",
        "    predicate='within'\n",
        ")\n",
        "\n",
        "# Create station -> NTA lookup\n",
        "station_nta_lookup = stations_with_nta.set_index('station_id')[['ntacode', 'ntaname', 'boroname']].to_dict('index')\n",
        "\n",
        "# Count stations per NTA\n",
        "nta_station_counts = stations_with_nta.groupby('ntacode').size()\n",
        "print(f'\\nNeighborhoods with Citi Bike stations: {len(nta_station_counts)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add NTA codes to trips\n",
        "def get_nta(station_id):\n",
        "    if pd.isna(station_id):\n",
        "        return None\n",
        "    info = station_nta_lookup.get(station_id)\n",
        "    return info['ntacode'] if info else None\n",
        "\n",
        "print('Mapping trips to neighborhoods...')\n",
        "trips['start_nta'] = trips['start_station_id'].map(lambda x: get_nta(x))\n",
        "trips['end_nta'] = trips['end_station_id'].map(lambda x: get_nta(x))\n",
        "\n",
        "# Remove trips that couldn't be mapped\n",
        "before = len(trips)\n",
        "trips = trips.dropna(subset=['start_nta'])\n",
        "print(f'Removed {before - len(trips):,} trips not in any neighborhood')\n",
        "print(f'Final trip count: {len(trips):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Aggregate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate departures by NTA, day of week, hour\n",
        "departures = trips.groupby(['start_nta', 'day_of_week', 'hour']).agg(\n",
        "    departures=('ride_id', 'count'),\n",
        "    member_departures=('is_member', 'sum'),\n",
        ").reset_index()\n",
        "\n",
        "# Aggregate arrivals\n",
        "arrivals = trips.groupby(['end_nta', 'day_of_week', 'hour']).agg(\n",
        "    arrivals=('ride_id', 'count'),\n",
        ").reset_index()\n",
        "arrivals.columns = ['nta', 'day_of_week', 'hour', 'arrivals']\n",
        "\n",
        "# Merge departures and arrivals\n",
        "departures.columns = ['nta', 'day_of_week', 'hour', 'departures', 'member_departures']\n",
        "hourly_stats = departures.merge(arrivals, on=['nta', 'day_of_week', 'hour'], how='outer').fillna(0)\n",
        "\n",
        "# Calculate net flow\n",
        "hourly_stats['net_flow'] = hourly_stats['arrivals'] - hourly_stats['departures']\n",
        "hourly_stats['member_pct'] = (hourly_stats['member_departures'] / hourly_stats['departures'] * 100).fillna(0)\n",
        "\n",
        "print(f'Aggregated {len(hourly_stats):,} rows (NTA × day × hour)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis A: Which neighborhoods wake up first?\n",
        "weekday_morning = hourly_stats[\n",
        "    (hourly_stats['day_of_week'] < 5) & \n",
        "    (hourly_stats['hour'] >= 5) & \n",
        "    (hourly_stats['hour'] <= 9)\n",
        "].groupby(['nta', 'hour'])['departures'].mean().reset_index()\n",
        "\n",
        "# Find peak hour for each neighborhood\n",
        "peak_hours = hourly_stats[\n",
        "    hourly_stats['day_of_week'] < 5  # Weekdays\n",
        "].groupby('nta').apply(\n",
        "    lambda x: x.loc[x['departures'].idxmax(), 'hour'] if len(x) > 0 else None\n",
        ").reset_index(name='peak_hour')\n",
        "\n",
        "print('Neighborhoods with earliest peak (6-7am):')\n",
        "early_risers = peak_hours[peak_hours['peak_hour'] <= 7]\n",
        "for _, row in early_risers.head(5).iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == row['nta']]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == row['nta']]) > 0 else row['nta']\n",
        "    print(f\"  {nta_name}: {int(row['peak_hour'])}:00\")\n",
        "\n",
        "print('\\nNeighborhoods with latest peak (after 9pm):')\n",
        "night_owls = peak_hours[peak_hours['peak_hour'] >= 21]\n",
        "for _, row in night_owls.head(5).iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == row['nta']]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == row['nta']]) > 0 else row['nta']\n",
        "    print(f\"  {nta_name}: {int(row['peak_hour'])}:00\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis B: Morning rush - which neighborhoods are exporters vs importers?\n",
        "morning_rush = hourly_stats[\n",
        "    (hourly_stats['day_of_week'] < 5) & \n",
        "    (hourly_stats['hour'] >= 8) & \n",
        "    (hourly_stats['hour'] <= 9)\n",
        "].groupby('nta').agg(\n",
        "    total_departures=('departures', 'sum'),\n",
        "    total_arrivals=('arrivals', 'sum'),\n",
        "    net_flow=('net_flow', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Top exporters (residential areas)\n",
        "print('Top 5 EXPORTERS (residential areas - people leaving for work):')\n",
        "exporters = morning_rush.nsmallest(5, 'net_flow')\n",
        "for _, row in exporters.iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == row['nta']]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == row['nta']]) > 0 else row['nta']\n",
        "    print(f\"  {nta_name}: {int(row['net_flow']):,} net flow\")\n",
        "\n",
        "# Top importers (office districts)\n",
        "print('\\nTop 5 IMPORTERS (office districts - people arriving for work):')\n",
        "importers = morning_rush.nlargest(5, 'net_flow')\n",
        "for _, row in importers.iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == row['nta']]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == row['nta']]) > 0 else row['nta']\n",
        "    print(f\"  {nta_name}: +{int(row['net_flow']):,} net flow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis C: Friday night vs Monday night\n",
        "friday_night = hourly_stats[\n",
        "    (hourly_stats['day_of_week'] == 4) &  # Friday\n",
        "    (hourly_stats['hour'] >= 22)\n",
        "].groupby('nta')['arrivals'].sum()\n",
        "\n",
        "monday_night = hourly_stats[\n",
        "    (hourly_stats['day_of_week'] == 0) &  # Monday\n",
        "    (hourly_stats['hour'] >= 22)\n",
        "].groupby('nta')['arrivals'].sum()\n",
        "\n",
        "night_comparison = pd.DataFrame({\n",
        "    'friday': friday_night,\n",
        "    'monday': monday_night\n",
        "}).fillna(0)\n",
        "night_comparison['ratio'] = night_comparison['friday'] / night_comparison['monday'].replace(0, 1)\n",
        "\n",
        "print('Neighborhoods with BIGGEST Friday night spike (vs Monday):')\n",
        "for nta, row in night_comparison.nlargest(5, 'ratio').iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == nta]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == nta]) > 0 else nta\n",
        "    print(f\"  {nta_name}: {row['ratio']:.1f}x more arrivals on Friday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis D: Weekend vs weekday transformation\n",
        "weekday_total = hourly_stats[\n",
        "    hourly_stats['day_of_week'] < 5\n",
        "].groupby('nta')['departures'].sum()\n",
        "\n",
        "weekend_total = hourly_stats[\n",
        "    hourly_stats['day_of_week'] >= 5\n",
        "].groupby('nta')['departures'].sum()\n",
        "\n",
        "weekend_comparison = pd.DataFrame({\n",
        "    'weekday': weekday_total / 5,  # Per day average\n",
        "    'weekend': weekend_total / 2   # Per day average\n",
        "}).fillna(0)\n",
        "weekend_comparison['ratio'] = weekend_comparison['weekend'] / weekend_comparison['weekday'].replace(0, 1)\n",
        "\n",
        "print('Neighborhoods that GROW most on weekends (likely parks/recreation):')\n",
        "for nta, row in weekend_comparison.nlargest(5, 'ratio').iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == nta]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == nta]) > 0 else nta\n",
        "    print(f\"  {nta_name}: {row['ratio']:.1f}x weekend vs weekday\")\n",
        "\n",
        "print('\\nNeighborhoods that SHRINK most on weekends (likely office districts):')\n",
        "for nta, row in weekend_comparison.nsmallest(5, 'ratio').iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == nta]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == nta]) > 0 else nta\n",
        "    print(f\"  {nta_name}: {row['ratio']:.2f}x weekend vs weekday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis E: Member % by neighborhood (tourist vs local)\n",
        "member_pct_by_nta = hourly_stats.groupby('nta').agg(\n",
        "    total_departures=('departures', 'sum'),\n",
        "    member_departures=('member_departures', 'sum')\n",
        ").reset_index()\n",
        "member_pct_by_nta['member_pct'] = member_pct_by_nta['member_departures'] / member_pct_by_nta['total_departures'] * 100\n",
        "\n",
        "print('Neighborhoods with LOWEST member % (likely tourist areas):')\n",
        "for _, row in member_pct_by_nta.nsmallest(5, 'member_pct').iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == row['nta']]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == row['nta']]) > 0 else row['nta']\n",
        "    print(f\"  {nta_name}: {row['member_pct']:.1f}% members\")\n",
        "\n",
        "print('\\nNeighborhoods with HIGHEST member % (likely residential):')\n",
        "for _, row in member_pct_by_nta.nlargest(5, 'member_pct').iterrows():\n",
        "    nta_name = stations_with_nta[stations_with_nta['ntacode'] == row['nta']]['ntaname'].iloc[0] if len(stations_with_nta[stations_with_nta['ntacode'] == row['nta']]) > 0 else row['nta']\n",
        "    print(f\"  {nta_name}: {row['member_pct']:.1f}% members\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Generate Output JSON Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File 1: neighborhoods.json (GeoJSON with metadata)\n",
        "\n",
        "# Get NTAs that have Citi Bike stations\n",
        "active_ntas = hourly_stats['nta'].unique()\n",
        "\n",
        "# Filter and simplify NTA geometries\n",
        "nta_filtered = nta[nta['ntacode'].isin(active_ntas)].copy()\n",
        "\n",
        "# Add computed properties\n",
        "nta_stats = hourly_stats.groupby('nta').agg(\n",
        "    total_departures=('departures', 'sum'),\n",
        "    avg_member_pct=('member_pct', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "nta_filtered = nta_filtered.merge(\n",
        "    nta_stats, \n",
        "    left_on='ntacode', \n",
        "    right_on='nta', \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Add peak hours\n",
        "nta_filtered = nta_filtered.merge(\n",
        "    peak_hours,\n",
        "    left_on='ntacode',\n",
        "    right_on='nta',\n",
        "    how='left',\n",
        "    suffixes=('', '_peak')\n",
        ")\n",
        "\n",
        "# Simplify geometries (reduce file size)\n",
        "nta_filtered['geometry'] = nta_filtered['geometry'].simplify(0.001)\n",
        "\n",
        "# Select columns for export\n",
        "export_columns = ['ntacode', 'ntaname', 'boroname', 'total_departures', 'avg_member_pct', 'peak_hour', 'geometry']\n",
        "nta_export = nta_filtered[[c for c in export_columns if c in nta_filtered.columns]]\n",
        "\n",
        "# Save as GeoJSON\n",
        "nta_export.to_file('output/neighborhoods.json', driver='GeoJSON')\n",
        "print(f'Saved neighborhoods.json ({os.path.getsize(\"output/neighborhoods.json\") / 1024:.1f} KB)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File 2: weekly-patterns.json\n",
        "\n",
        "# Restructure data: {nta: {day: [{h, dep, arr}]}}\n",
        "weekly_patterns = {}\n",
        "\n",
        "for nta in hourly_stats['nta'].unique():\n",
        "    nta_data = hourly_stats[hourly_stats['nta'] == nta]\n",
        "    weekly_patterns[nta] = {}\n",
        "    \n",
        "    for day in range(7):\n",
        "        day_data = nta_data[nta_data['day_of_week'] == day].sort_values('hour')\n",
        "        weekly_patterns[nta][str(day)] = [\n",
        "            {\n",
        "                'h': int(row['hour']),\n",
        "                'dep': int(row['departures']),\n",
        "                'arr': int(row['arrivals'])\n",
        "            }\n",
        "            for _, row in day_data.iterrows()\n",
        "        ]\n",
        "\n",
        "with open('output/weekly-patterns.json', 'w') as f:\n",
        "    json.dump(weekly_patterns, f, separators=(',', ':'))\n",
        "\n",
        "print(f'Saved weekly-patterns.json ({os.path.getsize(\"output/weekly-patterns.json\") / 1024:.1f} KB)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File 3: story-moments.json\n",
        "\n",
        "# Get NTA names lookup\n",
        "nta_names = stations_with_nta.drop_duplicates('ntacode').set_index('ntacode')['ntaname'].to_dict()\n",
        "\n",
        "# Calculate key statistics for story moments\n",
        "total_trips = len(trips)\n",
        "\n",
        "# Morning rush stats (8-9am weekdays)\n",
        "morning_rush_trips = trips[\n",
        "    (trips['day_of_week'] < 5) & \n",
        "    (trips['hour'] >= 8) & \n",
        "    (trips['hour'] <= 9)\n",
        "]\n",
        "morning_rush_count = len(morning_rush_trips)\n",
        "\n",
        "# Peak hour (highest departures overall)\n",
        "peak_hour_overall = hourly_stats.groupby('hour')['departures'].sum().idxmax()\n",
        "\n",
        "# Friday night top arrivals\n",
        "friday_top = night_comparison.nlargest(5, 'friday').index.tolist()\n",
        "friday_top_names = [nta_names.get(nta, nta) for nta in friday_top]\n",
        "\n",
        "# Weekend growers\n",
        "weekend_growers = weekend_comparison.nlargest(5, 'ratio').index.tolist()\n",
        "weekend_grower_names = [nta_names.get(nta, nta) for nta in weekend_growers]\n",
        "\n",
        "story_moments = {\n",
        "    'total_trips': total_trips,\n",
        "    'year': YEAR,\n",
        "    'morning_rush': {\n",
        "        'peak_time': f'{peak_hour_overall}:00',\n",
        "        'trips_8_9am': morning_rush_count,\n",
        "        'top_destinations': importers['nta'].head(3).apply(lambda x: nta_names.get(x, x)).tolist()\n",
        "    },\n",
        "    'friday_night': {\n",
        "        'top_arrivals': friday_top_names[:5],\n",
        "        'peak_hour': 23\n",
        "    },\n",
        "    'weekend': {\n",
        "        'top_growers': weekend_grower_names[:5]\n",
        "    },\n",
        "    'neighborhoods': {\n",
        "        'total_active': len(active_ntas),\n",
        "        'office_districts': importers['nta'].head(5).apply(lambda x: nta_names.get(x, x)).tolist(),\n",
        "        'residential': exporters['nta'].head(5).apply(lambda x: nta_names.get(x, x)).tolist()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('output/story-moments.json', 'w') as f:\n",
        "    json.dump(story_moments, f, indent=2)\n",
        "\n",
        "print(f'Saved story-moments.json ({os.path.getsize(\"output/story-moments.json\") / 1024:.1f} KB)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File 4: metadata.json\n",
        "\n",
        "metadata = {\n",
        "    'generated_at': pd.Timestamp.now().isoformat(),\n",
        "    'data_year': YEAR,\n",
        "    'data_months': list(MONTHS),\n",
        "    'total_trips': int(total_trips),\n",
        "    'unique_stations': len(stations),\n",
        "    'active_neighborhoods': len(active_ntas),\n",
        "    'files': {\n",
        "        'neighborhoods.json': f'{os.path.getsize(\"output/neighborhoods.json\") / 1024:.1f} KB',\n",
        "        'weekly-patterns.json': f'{os.path.getsize(\"output/weekly-patterns.json\") / 1024:.1f} KB',\n",
        "        'story-moments.json': f'{os.path.getsize(\"output/story-moments.json\") / 1024:.1f} KB'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('output/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f'Saved metadata.json')\n",
        "print(f'\\n=== SUMMARY ===')\n",
        "print(f\"Total output size: {sum(os.path.getsize(f'output/{f}') for f in os.listdir('output')) / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Download Output Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a zip file with all outputs\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive('nyc-bike-rhythms-data', 'zip', 'output')\n",
        "\n",
        "print('Downloading nyc-bike-rhythms-data.zip...')\n",
        "files.download('nyc-bike-rhythms-data.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Download the `nyc-bike-rhythms-data.zip` file\n",
        "2. Extract to your project's `/data/citibike/` folder\n",
        "3. Continue with the frontend implementation\n",
        "\n",
        "The output files should be under 3MB total and ready for static hosting."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
